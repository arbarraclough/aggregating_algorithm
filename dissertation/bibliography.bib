@article{kalnishkan:2022,
  title    = {Prediction with expert advice for a finite number of experts: A practical introduction},
  journal  = {Pattern Recognition},
  volume   = {126},
  pages    = {108557},
  year     = {2022},
  issn     = {0031-3203},
  doi      = {https://doi.org/10.1016/j.patcog.2022.108557},
  url      = {https://www.sciencedirect.com/science/article/pii/S0031320322000383},
  author   = {Y. Kalnishkan},
  keywords = {Online learning, Prediction, Model selection},
  abstract = {In this paper, prediction with expert advice is surveyed focusing on Vovk’s Aggregating Algorithm. The established theory as well as extensions developed in the recent decade are considered. The paper is aimed at practitioners and covers important application scenarios.}
}
@article{kalnishkan/vyugin:2008,
  title    = {The weak aggregating algorithm and weak mixability},
  journal  = {Journal of Computer and System Sciences},
  volume   = {74},
  number   = {8},
  pages    = {1228-1244},
  year     = {2008},
  note     = {Learning Theory 2005},
  issn     = {0022-0000},
  doi      = {https://doi.org/10.1016/j.jcss.2007.08.003},
  url      = {https://www.sciencedirect.com/science/article/pii/S0022000007001237},
  author   = {Y. Kalnishkan and M.V. Vyugin},
  keywords = {On-line learning, Predicting individual sequences, Prediction with expert advice, General loss functions},
  abstract = {This paper resolves the problem of predicting as well as the best expert up to an additive term of the order o(n), where n is the length of a sequence of letters from a finite alphabet. We call the games that permit this weakly mixable and give a geometrical characterisation of the class of weakly mixable games. Weak mixability turns out to be equivalent to convexity of the finite part of the set of superpredictions. For bounded games we introduce the Weak Aggregating Algorithm that allows us to obtain additive terms of the form Cn.}
}
@article{herbster/warmuth:1995,
  title   = {Tracking the Best Expert},
  author  = {M. Herbster and M.K. Warmuth},
  journal = {Machine Learning},
  year    = {1995},
  volume  = {32},
  pages   = {151-178},
  url     = {https://api.semanticscholar.org/CorpusID:12409091}
}
@article{vovk:2001,
  title   = {Competitive On-line Statistics},
  author  = {V. Vovk},
  journal = {International Statistical Review},
  year    = {2001},
  volume  = {69},
  url     = {https://api.semanticscholar.org/CorpusID:15491633}
}
@article{mitchell:2006,
  title  = {The Discipline of Machine Learning},
  author = {T.M. Mitchell},
  year   = {2006},
  month  = {01},
  pages  = {}
}
@article{freund:1997,
  author  = {Y.Freund and R. Schapire and Y. Singer and M.K. Warmuth},
  year    = {1997},
  month   = {01},
  pages   = {},
  title   = {Using and combining predictors that specialize},
  journal = {Conference Proceedings of the Annual ACM Symposium on Theory of Computing},
  doi     = {10.1145/258533.258616}
}
@article{vovk:1998,
  title    = {A Game of Prediction with Expert Advice},
  journal  = {Journal of Computer and System Sciences},
  volume   = {56},
  number   = {2},
  pages    = {153-173},
  year     = {1998},
  issn     = {0022-0000},
  doi      = {https://doi.org/10.1006/jcss.1997.1556},
  url      = {https://www.sciencedirect.com/science/article/pii/S0022000097915567},
  author   = {V. Vovk},
  abstract = {We consider the following problem. At each point of discrete time the learner must make a prediction; he is given the predictions made by a pool of experts. Each prediction and the outcome, which is disclosed after the learner has made his prediction, determine the incurred loss. It is known that, under weak regularity, the learner can ensure that his cumulative loss never exceedscL+alnn, wherecandaare some constants,nis the size of the pool, andLis the cumulative loss incurred by the best expert in the pool. We find the set of those pairs (c, a) for which this is true.}
}
@article{cesa-bianchi:1997,
  author     = {Cesa-Bianchi, Nicol\`{o} and Freund, Yoav and Haussler, David and Helmbold, David P. and Schapire, Robert E. and Warmuth, Manfred K.},
  title      = {How to use expert advice},
  year       = {1997},
  issue_date = {May 1997},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {44},
  number     = {3},
  issn       = {0004-5411},
  url        = {https://doi.org/10.1145/258128.258179},
  doi        = {10.1145/258128.258179},
  abstract   = {We analyze algorithms that predict a binary value by combining the predictions of several prediction strategies, called experts. Our analysis is for worst-case situations, i.e., we make no assumptions about the way the sequence of bits to be predicted is generated. We measure the performance of the algorithm by the difference between the expected number of mistakes it makes on the bit sequence and the expected number of mistakes made by the best expert on this sequence, where the expectation is taken with respect to the randomization in the predictins. We show that the minimum achievable difference is on the order of the square root of the number of mistakes of the best expert, and we give efficient algorithms that achieve this. Our upper and lower bounds have matching leading constants in most cases. We then show how this leads to certain kinds of pattern recognition/learning algorithms with performance bounds that improve on the best results currently know in this context. We also compare our analysis to the case in which log loss is used instead of the expected number of mistakes.},
  journal    = {J. ACM},
  month      = {may},
  pages      = {427–485},
  numpages   = {59},
  keywords   = {algorithms}
}
@incollection{vovk:1990,
  title     = {Aggregating Strategies},
  booktitle = {Colt Proceedings 1990},
  publisher = {Morgan Kaufmann},
  address   = {San Francisco},
  pages     = {371-383},
  year      = {1990},
  isbn      = {978-1-55860-146-8},
  doi       = {https://doi.org/10.1016/B978-1-55860-146-8.50032-1},
  url       = {https://www.sciencedirect.com/science/article/pii/B9781558601468500321},
  author    = {V. Vovk}
}
@inproceedings{kalnishkan:2015,
  author    = {Y. Kalnishkan and D. Adamskiy and A. Chernov and T. Scarfe},
  booktitle = {2015 IEEE International Conference on Data Mining Workshop (ICDMW)},
  title     = {Specialist Experts for Prediction with Side Information},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {1470-1477},
  doi       = {https://doi.org/10.1109/ICDMW.2015.161}
}
@book{box:2015,
  title     = {Time series analysis: forecasting and control},
  author    = {Box, George EP and Jenkins, Gwilym M and Reinsel, Gregory C and Ljung, Greta M},
  year      = {2015},
  publisher = {John Wiley \& Sons}
}
@article{adamskiy:2019,
  title     = {Aggregating algorithm for prediction of packs},
  author    = {Adamskiy, Dmitry and Bellotti, Anthony and Dzhamtyrova, Raisa and Kalnishkan, Yuri},
  journal   = {Machine Learning},
  volume    = {108},
  pages     = {1231--1260},
  year      = {2019},
  publisher = {Springer}
}
@techreport{kalnishkan:2009,
  institution = {Royal Holloway, University of London},
  title       = {The Aggregating Algorithm and Laissez-Faire Investment},
  author      = {Kalnishkan, Yuri},
  year        = {2009},
  number      = {CLRC-TR-09-02}
}
@article{blum:1997,
  title     = {Empirical support for winnow and weighted-majority algorithms: Results on a calendar scheduling domain},
  author    = {Blum, Avrim},
  journal   = {Machine Learning},
  volume    = {26},
  pages     = {5--23},
  year      = {1997},
  publisher = {Springer}
}
@book{reichenbach:1949,
  address   = {Berkeley},
  author    = {Reichenbach, Hans},
  editor    = {},
  publisher = {University of California Press},
  title     = {The Theory of Probability},
  year      = {1949}
}
@article{ross:1955,
  title     = {Randomization of a binary series},
  author    = {Ross, Bruce M},
  journal   = {The American journal of psychology},
  volume    = {68},
  number    = {1},
  pages     = {136--138},
  year      = {1955},
  publisher = {JSTOR}
}
@article{wagenaar:1970,
  title     = {Appreciation of conditional probabilities in binary sequences},
  author    = {Wagenaar, WA},
  journal   = {Acta Psychologica},
  volume    = {34},
  pages     = {348--356},
  year      = {1970},
  publisher = {Elsevier}
}
@article{bar-hillel:1991,
  title     = {The perception of randomness},
  author    = {Bar-Hillel, Maya and Wagenaar, Willem A},
  journal   = {Advances in applied mathematics},
  volume    = {12},
  number    = {4},
  pages     = {428--454},
  year      = {1991},
  publisher = {Elsevier}
}
@article{bakan:1960,
  title     = {Response-tendencies in attempts to generate random binary series},
  author    = {Bakan, Paul},
  journal   = {The American journal of psychology},
  volume    = {73},
  number    = {1},
  pages     = {127--131},
  year      = {1960},
  publisher = {JSTOR}
}
@article{lopes:1987,
  title     = {Distinguishing between random and nonrandom events.},
  author    = {Lopes, Lola L and Oden, Gregg C},
  journal   = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume    = {13},
  number    = {3},
  pages     = {392},
  year      = {1987},
  publisher = {American Psychological Association}
}
@inproceedings{kubovy:1988,
  title        = {More Random Than Random - a Study of Scaling Noises},
  author       = {Kubovy, M and Gilden, DL},
  booktitle    = {Bulletin of the Psychonomic Society},
  volume       = {26},
  pages        = {494--494},
  year         = {1988},
  organization = {Psychonomic Soc Inc 1710 Fortview Rd, Austin, TX 787704}
}
@article{nickerson:2009,
  title     = {On producing random binary sequences},
  author    = {Nickerson, Raymond S and Butler, Susan F},
  journal   = {The American Journal of Psychology},
  volume    = {122},
  number    = {2},
  pages     = {141--151},
  year      = {2009},
  publisher = {University of Illinois Press}
}
@incollection{mertens:1990,
  title     = {Repeated Games},
  editor    = {Tatsuro Ichiishi and Abraham Neyman and Yair Tauman},
  booktitle = {Game Theory and Applications},
  publisher = {Academic Press},
  address   = {San Diego},
  pages     = {77-130},
  year      = {1990},
  series    = {Economic Theory, Econometrics, and Mathematical Economics},
  isbn      = {978-0-12-370182-4},
  doi       = {https://doi.org/10.1016/B978-0-12-370182-4.50009-X},
  url       = {https://www.sciencedirect.com/science/article/pii/B978012370182450009X},
  author    = {Jean-François Mertens},
  abstract  = {Publisher Summary
               This chapter provides an overview of repeated games. It discusses the importance of the general model, many major applications requiring most of its features at the same time, and its conceptual interest. The theory postulates that all players know the full description of the game, and this description includes the payoff function of all his opponents, that is, in fact their utility function, their preferences. In a great many cases, this and other knowledge about the individual players' characteristics and strategic possibilities are not available to their opponents, who must, therefore, act according to some personal beliefs, prior distributions over those data. The general model is conceptually very simple and purely combinatorial equivalent. The chapter presents a few new tools and ideas that might be useful for further progress, illustrating at the same time some old ideas in a renewed framework as well as the high degree of interplay between various apparently unrelated topics within this general model.}
}
@article{littlestone:1994,
  title    = {The Weighted Majority Algorithm},
  journal  = {Information and Computation},
  volume   = {108},
  number   = {2},
  pages    = {212-261},
  year     = {1994},
  issn     = {0890-5401},
  doi      = {https://doi.org/10.1006/inco.1994.1009},
  url      = {https://www.sciencedirect.com/science/article/pii/S0890540184710091},
  author   = {N. Littlestone and M.K. Warmuth},
  abstract = {We study the construction of prediction algorithms in a situation in which a learner faces a sequence of trials, with a prediction to be made in each, and the goal of the learner is to make few mistakes. We are interested in the case where the learner has reason to believe that one of some pool of known algorithms will perform well, but the learner does not know which one. A simple and effective method, based on weighted voting, is introduced for constructing a compound algorithm in such a circumstance. We call this method the Weighted Majority Algorithm. We show that this algorithm is robust in the presence of errors in the data. We discuss various versions of the Weighted Majority Algorithm and prove mistake bounds for them that are closely related to the mistake bounds of the best algorithms of the pool. For example, given a sequence of trials, if there is an algorithm in the pool A that makes at most m mistakes then the Weighted Majority Algorithm will make at most c(log |A| + m) mistakes on that sequence, where c is fixed constant.}
}