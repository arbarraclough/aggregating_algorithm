\section{On-line Prediction \textit{(1,250)}}\label{section:On-line_Prediction}
% \section{On-line Machine Learning}
% \subsection{Concept \& Significance}
% \begin{quote}
%   \textbf{Explain the fundamental concepts of on-line prediction and why it is significant in various fields such as finance, weather forecasting, and machine learning. Discuss its impact on real-time data analysis and decision-making.}
% \end{quote}

% The term ``on-line machine learning'' refers to a method of making real-time updates to models based on sequential data, which has significant impacts on fields such as finance, weather forecasting, and healthcare. In order to fully understand the concept, it is essential that we first have a good understanding of the fundamentals of Machine Learning and its applications in dynamic environments.

% As Chair of Carnegie Mellon University's Machine Learning Department, Tom Mitchell, puts it, Machine Learning is a subset of artificial intelligence that ``is a natural outgrowth of the intersection of Computer Science and Statistics''\ \cite{mitchell:2006} that is concerned with developing algorithms and statistical models of the world that enable computers to perform tasks without the need for explicit commands, instead learning how to complete them purely from the data it is presented. Within this field, there exists many paradigms, each tailored to a specific problem type and dataset, including two that this report will focus on: ``batch learning'' and ``on-line learning''. 

% Unlike ``batch learning'', where the data that the algorithm is to learn from is readily available at the start of the training process, the order in which data is received by the model matters in ``on-line learning'' as it sequential. It is in this sequential nature of data that the significance of ``on-line learning'' lies; it allows for the continuous updating of models and predictions as new information arrives, without the need to completely retrain the model on the entire newly-updated training set. 


% \noindent\rule{\textwidth}{0.1pt}

% The key principles and mechanisms of on-line prediction include its real-time nature, where data is processed as it arrives, allowing for immediate predictions and updates. This capability is crucial for applications that require instant feedback and decisions. On-line prediction involves learning on the fly, a process where models are updated incrementally with each new data point. This is in contrast to traditional batch learning, where models are trained on a fixed dataset all at once and used for predictions without further updates until retrained with new data. Common algorithms used in on-line prediction, such as stochastic gradient descent, online versions of decision trees, and online support vector machines, are specifically designed to handle sequential data and update models incrementally. Evaluation of on-line prediction models typically employs metrics that can account for their adaptive nature, such as cumulative accuracy or regret.

% When comparing on-line learning with batch learning, the differences are clear. Batch learning involves training models on a fixed set of data all at once, with predictions made using this static model until it is retrained with new data. On-line learning, however, continuously updates the model with each new data point, making it more adaptable and suitable for environments where data is constantly changing.

% The advantages of on-line prediction are numerous. It offers adaptability, as models can quickly adjust to new information and changing patterns. It is efficient, eliminating the need for retraining models from scratch with new data. Moreover, it enables real-time decision-making, which is essential for applications requiring immediate responses, such as stock trading or fraud detection. However, on-line prediction also has its disadvantages. Implementing these systems can be technically challenging and resource-intensive, requiring significant computational power for continuous updating and processing. Additionally, without proper regularization, models may become overly sensitive to recent data points, leading to overfitting.

% Understanding these fundamental concepts highlights the critical role of on-line prediction in various fields and its profound impact on real-time data analysis and decision-making.

% \noindent\rule{\textwidth}{0.1pt}

% \begin{quote}
%   ``A computer program is said to learn from experience E with respect to some class
%   of tasks T and performance measure P, if its performance at tasks in T, as 
%   measured by P, improves with experience E.'' \textendash\ \textit{Mitchell, 1997}
% \end{quote}

% \subsection{Real-Time Decision-Making}
% \begin{quote}
%   Discuss how on-line prediction facilitates real-time decision-making processes. Provide examples of applications where immediate data processing and prediction are critical.
% \end{quote}

% \subsection{Challenges \& Opportunities}
% \begin{quote}
%   Analyse the main challenges associated with on-line prediction, such as data volatility, computational limitations, and algorithmic efficiency. Highlight potential opportunities for advancements in the field.
% \end{quote}

% \subsection{Role in Prediction with Expert Advice}
% \begin{quote}
%   Describe how on-line prediction integrates with expert advice to enhance decision-making accuracy. Discuss the synergy between real-time data processing and expert algorithms.
% \end{quote}

\subsection{Introduction (125)}
Within the scope of Machine Learning, there is a particular class of algorithms which can be used to make accurate predictions about the future based on past sequential data. One of the most notable among these is the ``Strong'' Aggregating Algorithm proposed by Volodymyr Vovk~\cite{vovk:1990} which is a powerful algorithm designed for On-line Prediction, specifically catering to the framework of Prediction with Expert Advice. The ``strong'' adjective is used purposefully in this context to distinguish it from the ``Weak'' Aggregating Algorithm proposed by Yuri Kalnishkan and Michael Vyugin~\cite{kalnishkan/vyugin:2008}. Given that the basis of discussion within this dissertation is focussed around this subject matter, this chapter aims to lay a comprehensive foundation of the key concepts before delving into them further in the chapters to come.

\subsection{Preliminaries (54)}
We begin by defining the On-line Prediction framework. On-line Prediction is a ``central problem in statistics and machine learning'' that is concerned with ``predicting future events based on past observations.''~\cite{cesa-bianchi:1997} It refers to a method in which a model makes predictions sequentially and updates its parameters in real-time as new data points become available.

\subsubsection*{On-line Prediction and Batch Learning (145)}
This marks the first distinction between on-line prediction and the traditional batch learning framework because on-line prediction requires approaches and algorithms that are significantly different. With batch learning, a whole training set of labelled examples $(x_i, y_i)$ is given to the learner at once in order to train a model. In contrast, on-line learning involves gradually feeding the learner information over time, requiring the model to continuously adapt to the new data it is given while requiring the learner to take actions on the basis of the information it already possesses instead of waiting for a complete picture. \cite{kalnishkan:2015} This forced adaptability ensures that the predictions outputted by the algorithm remain accurate based on the information that the model deems as relevant as it gains additional knowledge, making these models particularly valuable in applications that require immediate responses and fluidity such as financial market analysis and weather forecasting.

\subsubsection*{On-line Prediction and Timeseries Analysis (167)}
Another distinction that needs to be made is between on-line prediction and timeseries analysis as, while these are both ways of handling sequential data in machine learning and statistics, they are unique. On-line learning is based on processing data points sequentially and updating predictive models in real-time whereas timeseries analysis is based on modelling and forecasting data that is collected over successive time intervals. The prior approach does not impose any strict assumptions about the underlying data-generating process, even going so far as to not assume the existence of such a process~\cite{vovk:2001}, while the latter assumes a structured approach where observations are dependent on previous observations. These are typically modelled using stochastic processes such as \textit{autoregressive integrated moving average (ARIMA)} or \textit{state-space} models~\cite{box:2015}. The majority of the literature on On-line Prediction takes a similar stance that no assumptions can be made about the sequence of outcomes that are observed. Because of this, the analyses are done over the worst-case and may be better in reality~\cite{cesa-bianchi:1997}.

\subsubsection*{Notation (275)}
We can now introduce the commonly-used notation for On-line Prediction. 

Consider the scenario where the elements of a sequence called \textbf{\textit{outcomes}}  $\omega_t$ occur sequentially in discrete time $\omega_1, \omega_2, \ldots$ which we assume are drawn from an \textbf{\textit{outcome space}} $\Omega$ that is known beforehand. 

A learner is tasked with predicting these \textit{outcomes} one at a time by making \textbf{\textit{predictions}} $\gamma_t$ before they occur. Similarly, we assume that these \textit{predictions} are drawn from a known \textbf{\textit{prediction space}} $\Gamma$ which may or may not be the same as the \textit{outcome space} $\Omega$.

Once the learner has made their prediction, the true outcome is then revealed and the quality of the learner's prediction is assessed by a \textbf{\textit{loss function}} $\lambda(\gamma_t, \omega_t)$. This function measures the discrepancy between the prediction and outcome or, more generally, quantifies the effect of when prediction $\gamma_t$ is confronted with the outcome $\omega_t$~\cite{adamskiy:2019} by mapping the input space $\Gamma \times \Omega$ to a subset real-number line $\mathbb{R}$, typically $[0, +\infty)$. In this scenario, $\gamma_t$ can be thought of as the action taken in a situation of uncertainty and, as the uncertainty is lifted, the learner faces reality $\omega_t$ and faces the consequences of their action $\lambda(\gamma_t, \omega_t) : \Gamma\times\Omega\rightarrow[0,+\infty)$~\cite{kalnishkan:2009}.


\begin{protocol}[H]
    \caption{On-line Prediction Framework}\label{on-line_prediction_framework}
    \begin{algorithmic}[1]
        \State{FOR $t = 1, 2, \ldots$}
        \State{\hspace{\algorithmicindent} learner outputs $\gamma_t \in \Gamma$}
        \State{\hspace{\algorithmicindent} nature announces $\omega_t \in \Omega$}
        \State{\hspace{\algorithmicindent} learner suffers loss $\lambda(\gamma_t, \omega_t)$}
        \State{END FOR}
    \end{algorithmic}
\end{protocol}

Over multiple time steps $T$, the learner will suffer multiple losses which can be denoted as the cumulative loss up to time $T$. Their performance is measured by this cumulative loss, so a natural objective is to suffer as low a cumulative loss as it can.

\begin{equation}
    \text{Loss}_T(L) = \underset{t=1}{\overset{T}{\sum}}\lambda(\gamma_t, \omega_t)
\end{equation}

\subsubsection*{Games (200)}
A ``Game'' $\mathcal{G}$  is the choice of triple $<\Gamma, \Omega, \lambda>$ describing a \textbf{\textit{prediction environment.}} In this dissertation, we will only look at four common games—for more examples, see~\cite{vovk:1998}. \textit{Binary Games} is a term used for any game where $\Gamma = \Omega = [0, 1]$. In such scenarios, there are two common loss functions to be considered.
\begin{enumerate}
    \item \textbf{Square (Brier's) Loss} — $\lambda_\text{SQ}(\gamma, \omega) = {(\gamma - \omega)}^2$,
    \item \textbf{Absolute Loss} — $\lambda_\text{ABS}(\gamma, \omega) = {|\gamma - \omega |}^2$
\end{enumerate}

If we now consider the \textit{discrete binary game} where $\Gamma = [0, 1]$ and $\Omega = \{0, 1\}$, a logarithmic loss function can be used.
\begin{enumerate}
    \setcounter{enumi}{2}
    \item \textbf{Logarithmic Loss} — $\lambda_\text{LOG}(\gamma, \omega)=\begin{cases}-\ln(1-\gamma)&\text{if }\omega=0,\\-\ln\gamma&\text{if }\omega=1\end{cases}$
\end{enumerate}

Finally, there is the \textit{simple prediction game} where $\Gamma = \Omega = \{0, 1\}$, and loss function
\begin{enumerate}
\setcounter{enumi}{3}
    \item $\lambda(\gamma, \omega) = \begin{cases}0&\text{if }\omega=\gamma,\\1&\text{otherwise}\end{cases}$
\end{enumerate}

With such a game, the cumulative learner's loss $\text{Loss}_T(L)$ is equal to the number of mistakes made.

In this report, we are primarily concerned with the \textbf{\textit{Discrete Binary Square Loss Game}}, i.e. $\Gamma = [0, 1], \Omega = \{0, 1\}, \lambda_\text{SQ}(\gamma, \omega) = {(\gamma - \omega)}^2$

\subsection{Conclusion}